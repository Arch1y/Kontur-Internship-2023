{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install torchcontrib","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# IMPORT","metadata":{}},{"cell_type":"code","source":"import os\nimport torch\nimport pandas as pd\nimport torch.nn as nn\nimport numpy as np\nimport torch.nn.functional as F\nfrom torch.optim import lr_scheduler\nfrom torchcontrib.optim import SWA\n\nfrom sklearn import model_selection\nfrom sklearn import metrics\nimport transformers\nimport tokenizers\nfrom transformers import AdamW\nfrom transformers import get_linear_schedule_with_warmup\nfrom transformers import AutoModelForPreTraining, AutoTokenizer, AutoConfig\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\nfrom tqdm.autonotebook import tqdm\nimport utils","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# CONFIG","metadata":{}},{"cell_type":"code","source":"# Maximum length of the input vector.\nMAX_LEN = 1050\n# Batch size of training and validation set.\nTRAIN_BATCH_SIZE = 8\nVALID_BATCH_SIZE = 2\n# The number of epochs of model training.\nEPOCHS = 35\n# Name of the BERT model\nBERT_PATH = \"cointegrated/rubert-tiny2\"\n# Path to save the trained model.\nMODEL_SAVE_PATH = 'model.bin'\n# Path to training file.\nTRAINING_FILE = '/kaggle/input/kontur2023/nlp_test_task_2023/nlp_test_task_2023/dataset/train.json'\n# Tokenizer for ruBERT-tiny2 model.\nTOKENIZER = AutoTokenizer.from_pretrained(\"cointegrated/rubert-tiny2\", lowercase=True)\n# Label regularization.\nSOFT_ALPHA = 0.4","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# PROCESS DATA","metadata":{}},{"cell_type":"code","source":"def process_data(text, extracted_part, start_index, end_index, label, tokenizer, max_len):\n    '''Creating arrays with tokens, offsets and masks for the BERT model.\n    \n    Args:\n        text: The original text of the document.\n        extracted_part: The original extracted part of the text alias the correct answer.\n        start_index: The character index of the start of the correct answer.\n        end_index: The character index of the end of the correct answer.\n        label: Text labels.\n        tokenizer: The tokenizer to use for the BERT model.\n        max_len: The length of the longest sentence after tokenization, including special tokens.\n        \n    Returns:\n        dictionary with keys:\n            ids: Output ids for the BERT model for the Q&A problem.\n            mask: The attention mask\n            token_type_ids: The type of input tokens to split the array into \"label\" aka question and \"text\".\n            targets_start: The token index of the start of the correct answer.\n            targets_end: The token index of the end of the correct answer.\n            orig_text: The original text.\n            orig_extracted: The original extracted part.\n            label: The label of the original text.\n            offsets: The beginning and end of each word in a sentence.\n    '''\n    character_targets = [0] * len(text)\n    # [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] \n    if start_index != 0 and end_index != 0:\n        for ct in range(start_index, end_index + 1):\n            character_targets[ct] = 1\n    # [0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0]\n    # Where 1 is the target values, what should be extracted.\n    \n    # Create tokenizer.\n    tok_text = tokenizer.encode_plus(text, return_offsets_mapping=True)\n    # Input ids of tokens from input text and delete special tokens ([CLS], [SEP]).\n    input_ids_orig = tok_text.input_ids[1:-1]\n    # Save offsets of words in the text without tokens.\n    text_offsets = tok_text.offset_mapping[1:-1]\n    \n    # Create targets for words with offset_mapping.\n    target_idx = []\n    for k, (offset_1, offset_2) in enumerate(text_offsets):\n        if sum(character_targets[offset_1: offset_2]) > 0:\n            target_idx.append(k)\n    \n    # Check if there is an answer in the text.\n    if len(target_idx) > 0:\n        targets_start = target_idx[0]\n        targets_end = target_idx[-1]\n    else:\n        targets_start = 0\n        targets_end = 0\n    # There are target arrays like:\n    # [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0] - target start\n    # [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0] - target end\n    \n    # ids of tokens for labels of the texts.\n    label_id = {\n        'обеспечение исполнения контракта': [33231, 32922, 36035],\n        'обеспечение гарантийных обязательств': [33231, 55482, 2313, 38970],   \n    }\n    # Soft targets\n    # [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0] - target start before.\n    # [0, 0, 0, 0, 0, 0.3, 0.6, 1, 0, 0, 0, 0, 0] - after regularization.\n    n = len(input_ids_orig)\n    sentence = np.arange(n)\n    answer = sentence[targets_start:targets_end + 1]\n    \n    start_labels = np.zeros(n)\n    for i in range(targets_end+1):\n        jac = utils.jaccard_array(answer, sentence[i:targets_end + 1])\n        start_labels[i] = jac\n    start_labels = (1 - SOFT_ALPHA) * start_labels / start_labels.sum()\n    start_labels[targets_start] += SOFT_ALPHA\n    \n    end_labels = np.zeros(n)\n    for i in range(targets_start, n):\n        jac = utils.jaccard_array(answer, sentence[targets_start:i + 1])\n        end_labels[i] = jac\n    end_labels = (1 - SOFT_ALPHA) * end_labels / end_labels.sum()\n    end_labels[targets_end] += SOFT_ALPHA\n    \n    # We format the arrays for the Q&A format for BERT:\n    # [CLS] label_id [SEP] text [SEP].\n    # And, since we added special tokens, we must make the appropriate offsets in all other arrays.\n    # We add +5 for 'обеспечение исполнения контракта' label and +6 for 'обеспечение гарантийных обязательств'\n    # because their length differs by one token.\n    # [CLS] + len(label) + [SEP] = 5 or 6!\n    if label == 'обеспечение исполнения контракта':\n        input_ids = [2] + [*label_id[label]] + [3] + input_ids_orig + [3]\n        token_type_ids = [0, 0, 0, 0, 0] + [1] * (len(input_ids_orig) + 1)\n        mask = [1] * len(token_type_ids)\n        text_offsets = [(0, 0)] * 5 + text_offsets + [(0, 0)]\n        start_labels = [0, 0, 0, 0, 0] + list(start_labels) + [0]\n        end_labels = [0, 0, 0, 0, 0] + list(end_labels) + [0]\n        targets_start += 5\n        targets_end += 5\n    else:\n        input_ids = [2] + [*label_id[label]] + [3] + input_ids_orig + [3]\n        token_type_ids = [0, 0, 0, 0, 0, 0] + [1] * (len(input_ids_orig) + 1)\n        mask = [1] * len(token_type_ids)\n        text_offsets = [(0, 0)] * 6 + text_offsets + [(0, 0)]\n        start_labels = [0, 0, 0, 0, 0, 0] + list(start_labels) + [0]\n        end_labels = [0, 0, 0, 0, 0, 0] + list(end_labels) + [0]\n        targets_start += 6\n        targets_end += 6\n    \n    # We adjust the length of the input vectors to the max_len parameter.\n    padding_length = max_len - len(input_ids)\n    if padding_length > 0:\n        input_ids = input_ids + ([0] * padding_length)\n        mask = mask + ([0] * padding_length)\n        token_type_ids = token_type_ids + ([0] * padding_length)\n        text_offsets = text_offsets + ([(0, 0)] * padding_length)\n        start_labels = start_labels + ([0] * padding_length)\n        end_labels = end_labels + ([0] * padding_length)\n        \n    return {\n        'ids': input_ids,\n        'mask': mask,\n        'token_type_ids': token_type_ids,\n        'targets_start': start_labels,\n        'targets_end': end_labels,\n        'orig_text': text,\n        'orig_extracted': extracted_part,\n        'label': label,\n        'offsets': text_offsets\n    }","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# LOADING DATA","metadata":{}},{"cell_type":"code","source":"class TextLoading:\n    '''Loading data in the correct form in the model.\n    '''\n    def __init__(self, text, label, extracted_part, start_index, end_index):\n        '''Initializes the values for data preparation.\n        \n        Args:\n            text: The original text of the document.\n            label: Text labels.\n            extracted_part: The original extracted part of the text alias the correct answer.\n            start_index: The character index of the start of the correct answer.\n            end_index: The character index of the end of the correct answer.\n        '''\n        self.text = text\n        self.label = label\n        self.extracted_part = extracted_part\n        self.start_index = start_index\n        self.end_index = end_index\n        self.tokenizer = TOKENIZER\n        self.max_len = MAX_LEN\n    \n    def __len__(self):\n        '''Return len of the text\n        '''\n        return len(self.text)\n    \n    def __getitem__(self, item):\n        data = process_data(\n            self.text[item], \n            self.extracted_part[item],\n            self.start_index[item],\n            self.end_index[item],\n            self.label[item],\n            self.tokenizer,\n            self.max_len\n        )\n        # Return the processed data converted to torch.tensor format.\n        return {\n            'ids': torch.tensor(data[\"ids\"], dtype=torch.long),\n            'mask': torch.tensor(data[\"mask\"], dtype=torch.long),\n            'token_type_ids': torch.tensor(data[\"token_type_ids\"], dtype=torch.long),\n            'targets_start': torch.tensor(data[\"targets_start\"], dtype=torch.float),\n            'targets_end': torch.tensor(data[\"targets_end\"], dtype=torch.float),\n            'orig_text': data[\"orig_text\"],\n            'orig_extracted': data[\"orig_extracted\"],\n            'label': data[\"label\"],\n            'offsets': torch.tensor(data[\"offsets\"], dtype=torch.long)\n        }","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# NEURAL NETWORK MODEL","metadata":{}},{"cell_type":"code","source":"class TextModel(transformers.BertPreTrainedModel):\n    '''Neural network model'''\n    def __init__(self, conf):\n        super(TextModel, self).__init__(conf)\n        # Create backbone of BERT model.\n        self.bert = AutoModelForPreTraining.from_pretrained(BERT_PATH, config=conf)\n        # Dropout with 50% probability.\n        self.drop_out_high = nn.Dropout(0.5)\n        # Create linear output with size 312 (size of BERT output).\n        # Multiplication by 2 is necessary because the last two layers are taken.\n        self.classifier = nn.Linear(conf.hidden_size * 2, 2)\n        # Initialization of weights for linear layer.\n        torch.nn.init.normal_(self.classifier.weight, std=0.02)\n    \n    def forward(self, ids, mask, token_type_ids):\n        '''Direct pass through the NN.\n        '''\n        # Return the hidden states from the BERT model.\n        out = self.bert(ids,\n                        attention_mask=mask,\n                        token_type_ids=token_type_ids)\n        \n        # Concatenate all hidden layers.\n        # We take not olny the last layer, because the last one can be \n        # overfitted on the original training data.\n        out = torch.stack(tuple(out.hidden_states[-i -1] for i in range(3)), dim=0)\n        out_mean = torch.mean(out, dim=0)\n        out_max, _ = torch.max(out, dim=0)\n        out = torch.cat((out_mean, out_max), dim=-1)\n        # Multiple dropout on output layer to avoid overfitting.\n        logits = torch.mean(torch.stack([self.classifier(self.drop_out_high(out)) for _ in range(5)], dim=0), dim=0)\n        \n        # Split the vector to start and end logits - start probabilities \n        # and end probabilities of the correct answer.\n        start_logits, end_logits = logits.split(1, dim=-1)\n\n        start_logits = start_logits.squeeze(-1)\n        end_logits = end_logits.squeeze(-1)\n\n        return start_logits, end_logits","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# LOSS FUNCTION AND JACCARD SCORE","metadata":{}},{"cell_type":"code","source":"def loss_fn(start_logits, end_logits, start_positions, end_positions):\n    '''KLDiv Loss for start and end logits probabilities\n    '''\n    m = torch.nn.LogSoftmax(dim=1)\n    loss_fct = torch.nn.KLDivLoss(reduction='batchmean')\n    start_loss = loss_fct(m(start_logits), start_positions)\n    end_loss = loss_fct(m(end_logits), end_positions)\n    total_loss = (start_loss + end_loss)\n    return total_loss\n\n\ndef calculate_jaccard_score(original_text, target_string, label_val, \n                            idx_start, idx_end, offsets):\n    '''Jaccard similarity calculation for original text and prediction.\n    \n    Args:\n        original_text: The original text of the document.\n        target_string: Target string (extracted part).\n        label_val: The label of the text.\n        idx_start: The predicted index of the start of the string.\n        idx_end: The predicted index of the end of the string.\n        offsets: Offsets of the text after tokenization.\n    \n    Returns:\n        jac: The score of jac similarity.\n        filtered_output: The string of the prediction text.\n        idx_start: The predicted index of the start of the string after filtration \n        (idx_start = 0 if start >= end. It's mean that BERT cannot find the target string).\n        idx_end: The predicted index of the end of the string after filtration.\n    '''\n    \n    # Return the target string from idx_start/end.\n    filtered_output  = \"\"\n    for ix in range(idx_start, idx_end + 1):\n        filtered_output += original_text[offsets[ix][0]: offsets[ix][1]]\n        # Add space between words, if token isn't the last one.\n        if (ix+1) < len(offsets) and offsets[ix][1] < offsets[ix+1][0]:\n            filtered_output += \" \"\n    \n    \n    # Often, the model gives the first word or letter of the first word \n    # as an answer, although in fact there is no answer in the text.\n    # To avoid this, the following selection method is used. \n    # Since real answers are much longer than 4 words.\n    if len(filtered_output.split()) < 4:\n        filtered_output = ''\n        char_start = char_end = 0\n    else:\n        char_start = offsets[(idx_start)][0]\n        char_end = offsets[(idx_end)][1]\n    # It may help to remove extra punctuation marks at the end of the answer.\n#     filtered_output = filtered_output.rstrip(' ,/:')\n\n    # Calculate the jaccard score and accuracy between\n    # the predicted text and the original one.\n    jac = utils.jaccard(target_string.strip(), filtered_output.strip())\n    acc = utils.accuracy(target_string.strip(), filtered_output.strip())\n    return jac, filtered_output, char_start, char_end, acc","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# TRAIN","metadata":{}},{"cell_type":"code","source":"def train_fn(data_loader, model, optimizer, device, scheduler=None):\n    '''Training the BERT model.\n    \n    Args:\n        data_loader: data loading utility by PyTorch.\n        model: model of the neural network.\n        optimizer: AdamW optimizer to optimize the required parameters. \n        device: CPU/GPU.\n        scheduler: Training schedule to control the speed of learning.\n        \n    '''\n    # Starting to train the model.\n    model.train()\n    losses = utils.AverageMeter()\n    jaccards = utils.AverageMeter()\n    accuracy = utils.AverageMeter()\n    \n    # tqdm to visualize the learning process.\n    tk0 = tqdm(data_loader, total=len(data_loader))\n    \n    for bi, d in enumerate(tk0):\n        \n        ids = d[\"ids\"]\n        token_type_ids = d[\"token_type_ids\"]\n        mask = d[\"mask\"]\n        targets_start = d[\"targets_start\"]\n        targets_end = d[\"targets_end\"]\n        label = d[\"label\"]\n        orig_extracted = d[\"orig_extracted\"]\n        orig_text = d[\"orig_text\"]\n        offsets = d[\"offsets\"]\n        \n        # We transfer the tensors to the device (CPU/GPU)\n        ids = ids.to(device, dtype=torch.long)\n        token_type_ids = token_type_ids.to(device, dtype=torch.long)\n        mask = mask.to(device, dtype=torch.long)\n        targets_start = targets_start.to(device, dtype=torch.float)\n        targets_end = targets_end.to(device, dtype=torch.float)\n        \n        # Reset the gradients at the beginning of model training.\n        model.zero_grad()\n        # Move ids, mask, token_type_ids to the model and \n        # Predict logits start and end values.\n        outputs_start, outputs_end = model(\n            ids=ids,\n            mask=mask,\n            token_type_ids=token_type_ids,\n        )\n        # Calculate the loss function.\n        loss = loss_fn(outputs_start, outputs_end, targets_start, targets_end)\n        # Сalculation gradients from loss function.\n        loss.backward()\n        # Updating weight parameters\n        optimizer.step()\n        # Update scheduler => update learning rate\n        scheduler.step()\n        \n        # Applying the softmax function to obtain pseudo probabilities for start and end tokens.\n        outputs_start = torch.softmax(outputs_start, dim=1).cpu().detach().numpy()\n        outputs_end = torch.softmax(outputs_end, dim=1).cpu().detach().numpy()\n        # Jaccard and accuracy calculation for the epoch.\n        jaccard_scores = []\n        accuracy_scores = []\n        for px, text in enumerate(orig_text):\n            extracted_text = orig_extracted[px]\n            text_label = label[px]\n            jaccard_score, _, _, _, accuracy_score = calculate_jaccard_score(\n                                                    original_text=text,\n                                                    target_string=extracted_text,\n                                                    label_val=text_label,\n                                                    idx_start=np.argmax(outputs_start[px, :]),\n                                                    idx_end=np.argmax(outputs_end[px, :]),\n                                                    offsets=offsets[px]\n                                                )\n            jaccard_scores.append(jaccard_score)\n            accuracy_scores.append(accuracy_score)\n        \n        # Update losses, accuracy and jaccard\n        accuracy.update(np.mean(accuracy_scores), ids.size(0))\n        jaccards.update(np.mean(jaccard_scores), ids.size(0))\n        losses.update(loss.item(), ids.size(0))\n        # tqdm loading bar with updated losses, accuracy \n        # and jaccard at each epoch.\n        tk0.set_postfix(loss=losses.avg, jaccard=jaccards.avg, accuracy=accuracy.avg)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# VALIDATION","metadata":{}},{"cell_type":"code","source":"def eval_fn(data_loader, model, device):\n    '''Validation the BERT model.\n    \n    Args:\n        data_loader: data loading utility by PyTorch.\n        model: model of the neural network.\n        optimizer: AdamW optimizer to optimize the required parameters. \n        device: CPU/GPU.\n        scheduler: Training schedule to control the speed of learning.\n        \n    '''\n    # Starting the model validation process: turn off dropout.\n    model.eval()\n    losses = utils.AverageMeter()\n    jaccards = utils.AverageMeter()\n    accuracy = utils.AverageMeter()\n    \n    # Turn off gradient calculation.\n    with torch.no_grad():\n        tk0 = tqdm(data_loader, total=len(data_loader))\n        for bi, d in enumerate(tk0):\n            ids = d[\"ids\"]\n            token_type_ids = d[\"token_type_ids\"]\n            mask = d[\"mask\"]\n            label = d[\"label\"]\n            orig_extracted = d[\"orig_extracted\"]\n            orig_text = d[\"orig_text\"]\n            targets_start = d[\"targets_start\"]\n            targets_end = d[\"targets_end\"]\n            offsets = d[\"offsets\"].numpy()\n            \n            # Move tensors to device (CPU/GPU)\n            ids = ids.to(device, dtype=torch.long)\n            token_type_ids = token_type_ids.to(device, dtype=torch.long)\n            mask = mask.to(device, dtype=torch.long)\n            targets_start = targets_start.to(device, dtype=torch.float)\n            targets_end = targets_end.to(device, dtype=torch.float)\n            \n            # Predict start and the end indexes of the model.\n            outputs_start, outputs_end = model(\n                ids=ids,\n                mask=mask,\n                token_type_ids=token_type_ids\n            )\n            \n            # Calculation loss function.\n            loss = loss_fn(outputs_start, outputs_end, targets_start, targets_end)\n            \n            # Softmax function for probabilities.\n            outputs_start = torch.softmax(outputs_start, dim=1).cpu().detach().numpy()\n            outputs_end = torch.softmax(outputs_end, dim=1).cpu().detach().numpy()\n            # Jaccard and accuracy score calculation.\n            jaccard_scores = []\n            accuracy_scores = []\n            for px, text in enumerate(orig_text):\n                extracted_text = orig_extracted[px]\n                text_label = label[px]\n                jaccard_score, _, _, _, accuracy_score = calculate_jaccard_score(\n                    original_text=text,\n                    target_string=extracted_text,\n                    label_val=text_label,\n                    idx_start=np.argmax(outputs_start[px, :]),\n                    idx_end=np.argmax(outputs_end[px, :]),\n                    offsets=offsets[px]\n                )\n                jaccard_scores.append(jaccard_score)\n                accuracy_scores.append(accuracy_score)\n            \n            # Update jaccard, accuracy and losses.\n            accuracy.update(np.mean(accuracy_scores), ids.size(0))\n            jaccards.update(np.mean(jaccard_scores), ids.size(0))\n            losses.update(loss.item(), ids.size(0))\n            # Display scores.\n            tk0.set_postfix(loss=losses.avg, jaccard=jaccards.avg, accuracy=accuracy.avg)\n    return jaccards.avg, accuracy.avg","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# RUN MODEL","metadata":{}},{"cell_type":"code","source":"def run(fold):\n    \n    # Read the training file and get it ready to go.\n    dfx = pd.read_json(TRAINING_FILE)\n    splitted_df = pd.json_normalize(dfx.extracted_part)\n    splitted_df['extracted_part'] = splitted_df['text']\n    for i in ['extracted_part', 'answer_start', 'answer_end']:\n        dfx[i] = splitted_df[i].str[0]\n    # Add column with kfold for cross validation.\n    dfx['kfold'] = -1\n    \n    # Since the objective function is two peaks at zero and around a certain value.\n    # Therefore, it is necessary to correctly divide the data set into equal groups:\n    # train and valid which contain samples from these two clusters in approximately \n    # the same amount. This makes it possible not to overfit the model only on the \n    # sample with answer start = 0, or another group of samples.\n\n    dfx_temp = dfx.loc[dfx['answer_start'] != 0]\n    dfx['q_answer_start'] = pd.qcut(dfx_temp['answer_start'], q=2).astype('str')\n    dfx['q_answer_start'] = dfx['q_answer_start'].replace(np.nan, 0).astype('str')\n    dfx['q_answer_start_label'] = dfx.q_answer_start.str.cat(dfx.label)\n    kf = model_selection.StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n    y = dfx[\"q_answer_start_label\"].values\n    \n    # Create 5 folders for 5 models. In the future, we will average the output values\n    # across all models, which in theory will give a better result than if we trained \n    # a single model.\n    for kfold, (train_idx, valid_idx) in enumerate(kf.split(X=dfx, y=y)):\n        dfx.loc[valid_idx, \"kfold\"] = kfold\n\n    # create fold from dfx 0,1,2,3,4.\n    df_train = dfx[dfx.kfold != fold].reset_index(drop=True)\n    df_valid = dfx[dfx.kfold == fold].reset_index(drop=True)\n    \n    # Create train dataset from TextLoading.\n    train_dataset = TextLoading(\n        text=df_train.text.values,\n        label=df_train.label.values,\n        extracted_part=df_train.extracted_part.values,\n        start_index=df_train.answer_start.values,\n        end_index=df_train.answer_end.values\n    )\n    \n    # Creating a generator to give out a data set in batches.\n    train_data_loader = torch.utils.data.DataLoader(\n        train_dataset,\n        batch_size=TRAIN_BATCH_SIZE,\n        num_workers=2\n    )\n    \n    # Create valid dataset from TextLoading.\n    valid_dataset = TextLoading(\n        text=df_valid.text.values,\n        label=df_valid.label.values,\n        extracted_part=df_valid.extracted_part.values,\n        start_index=df_valid.answer_start.values,\n        end_index=df_valid.answer_end.values\n    )\n    \n    # Creating a generator to give out a data set in batches.\n    valid_data_loader = torch.utils.data.DataLoader(\n        valid_dataset,\n        batch_size=VALID_BATCH_SIZE,\n        num_workers=1\n    )\n\n    # Turn on GPU for calculation gradients.\n    device = torch.device(\"cuda\")\n    # Loading pretrained BERT model.\n    model_config = AutoConfig.from_pretrained(BERT_PATH)\n    # Output hidden states ON for concatenate the hidden states\n    # of BERT model.\n    model_config.output_hidden_states = True\n    model = TextModel(conf=model_config)\n    # Move the model to the GPU.\n    model.to(device)\n\n    # Calculation the number of training steps.\n    num_train_steps = int(len(df_train) / TRAIN_BATCH_SIZE * EPOCHS)\n    # Get model parameters.\n    param_optimizer = list(model.named_parameters())\n    # Parameters that we don't won't to change.\n    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n    # Creat two sets of parameters with weight decay = 0 and \n    # with weight decay =/= 0 for update it\n    optimizer_parameters = [\n        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.001},\n        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0},\n    ]\n    # Create AdamW optimizer with our parameters.\n    base_opt = transformers.AdamW(optimizer_parameters, lr=7e-5, no_deprecation_warning=True)\n    # Сreating SWA on top of AdamW for better accuracy on the validation set.\n    optimizer = SWA(base_opt, swa_start=int(num_train_steps * 0.9),\n                                       swa_freq=30, swa_lr=None)\n    \n    # Creating a schedule to control the learning rate during training. \n    # This schedule have got a learning rate that decreases linearly at each training step.\n    scheduler = get_linear_schedule_with_warmup(\n        optimizer, \n        num_warmup_steps=int(num_train_steps * 0.25), \n        num_training_steps=num_train_steps\n    )\n    \n    # Create early stopping function with patience = 5.\n    # This means that the learning process will stop after 5 unsuccessful epochs.\n    es = utils.EarlyStopping(patience=5, mode=\"max\")\n    print(f\"Training is Starting for fold={fold}\")\n    \n    # Training the model.\n    for epoch in range(EPOCHS):\n        train_fn(train_data_loader, model, optimizer, device, scheduler=scheduler)\n        jaccard, accuracy = eval_fn(valid_data_loader, model, device)\n        print(f\"Accuracy score = {accuracy}\", f\"\\nJaccard Score = {jaccard}\")\n        es(accuracy, model, model_path=f\"model_{fold}.bin\")\n        if es.early_stop:\n            print(\"Early stopping\")\n            break\n    return accuracy","metadata":{"execution":{"iopub.status.busy":"2023-04-23T10:23:57.718674Z","iopub.execute_input":"2023-04-23T10:23:57.720363Z","iopub.status.idle":"2023-04-23T10:23:57.740848Z","shell.execute_reply.started":"2023-04-23T10:23:57.720307Z","shell.execute_reply":"2023-04-23T10:23:57.739661Z"},"trusted":true},"execution_count":98,"outputs":[]},{"cell_type":"markdown","source":"# Create 5-fold of learning","metadata":{}},{"cell_type":"code","source":"accuracy = []","metadata":{"execution":{"iopub.status.busy":"2023-04-23T10:19:34.747558Z","iopub.execute_input":"2023-04-23T10:19:34.750259Z","iopub.status.idle":"2023-04-23T10:19:34.758799Z","shell.execute_reply.started":"2023-04-23T10:19:34.750188Z","shell.execute_reply":"2023-04-23T10:19:34.755286Z"},"trusted":true},"execution_count":96,"outputs":[]},{"cell_type":"code","source":"accuracy.append(run(fold=0))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"accuracy.append(run(fold=1))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"accuracy.append(run(fold=2))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"accuracy.append(run(fold=3))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"accuracy.append(run(fold=4))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sum(accuracy) / len(accuracy)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# TEST THE MODEL","metadata":{}},{"cell_type":"code","source":"device = torch.device(\"cuda\")\nmodel_config = AutoConfig.from_pretrained(BERT_PATH)\nmodel_config.output_hidden_states = True","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model1 = TextModel(conf=model_config)\nmodel1.to(device)\nmodel1.load_state_dict(torch.load(\"/kaggle/working/model_0.bin\"))\nmodel1.eval()\n\nmodel2 = TextModel(conf=model_config)\nmodel2.to(device)\nmodel2.load_state_dict(torch.load(\"/kaggle/working/model_1.bin\"))\nmodel2.eval()\n\nmodel3 = TextModel(conf=model_config)\nmodel3.to(device)\nmodel3.load_state_dict(torch.load(\"/kaggle/working/model_2.bin\"))\nmodel3.eval()\n\nmodel4 = TextModel(conf=model_config)\nmodel4.to(device)\nmodel4.load_state_dict(torch.load(\"/kaggle/working/model_3.bin\"))\nmodel4.eval()\n\nmodel5 = TextModel(conf=model_config)\nmodel5.to(device)\nmodel5.load_state_dict(torch.load(\"/kaggle/working/model_4.bin\"))\nmodel5.eval()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dfx_test = pd.read_json('/kaggle/input/kontur2023/nlp_test_task_2023/nlp_test_task_2023/dataset/test.json')\ndfx_test.loc[:, 'extracted_part'] = dfx_test.text.values\ndfx_test.loc[:, 'answer_start'] = 0\ndfx_test.loc[:, 'answer_end'] = dfx_test.text.str.len() ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_output = []\n\ntest_dataset = TextLoading(\n        text=dfx_test.text.values,\n        label=dfx_test.label.values,\n        extracted_part=dfx_test.extracted_part.values,\n        start_index=dfx_test.answer_start.values,\n        end_index=dfx_test.answer_end.values\n    )\n\ndata_loader = torch.utils.data.DataLoader(\n    test_dataset,\n    shuffle=False,\n    batch_size=4,\n    num_workers=1\n)\n\nwith torch.no_grad():\n    tk0 = tqdm(data_loader, total=len(data_loader))\n    for bi, d in enumerate(tk0):\n        ids = d[\"ids\"]\n        token_type_ids = d[\"token_type_ids\"]\n        mask = d[\"mask\"]\n        label = d[\"label\"]\n        orig_extracted = d[\"orig_extracted\"]\n        orig_text = d[\"orig_text\"]\n        targets_start = d[\"targets_start\"]\n        targets_end = d[\"targets_end\"]\n        offsets = d[\"offsets\"].numpy()\n\n        ids = ids.to(device, dtype=torch.long)\n        token_type_ids = token_type_ids.to(device, dtype=torch.long)\n        mask = mask.to(device, dtype=torch.long)\n        targets_start = targets_start.to(device, dtype=torch.long)\n        targets_end = targets_end.to(device, dtype=torch.long)\n\n        outputs_start1, outputs_end1 = model1(\n            ids=ids,\n            mask=mask,\n            token_type_ids=token_type_ids\n        )\n        \n        outputs_start2, outputs_end2 = model2(\n            ids=ids,\n            mask=mask,\n            token_type_ids=token_type_ids\n        )\n        \n        outputs_start3, outputs_end3 = model3(\n            ids=ids,\n            mask=mask,\n            token_type_ids=token_type_ids\n        )\n        \n        outputs_start4, outputs_end4 = model4(\n            ids=ids,\n            mask=mask,\n            token_type_ids=token_type_ids\n        )\n        \n        outputs_start5, outputs_end5 = model5(\n            ids=ids,\n            mask=mask,\n            token_type_ids=token_type_ids\n        )\n        outputs_start = (\n            outputs_start1 \n            + outputs_start2 \n            + outputs_start3 \n            + outputs_start4 \n            + outputs_start5\n        ) / 5\n        outputs_end = (\n            outputs_end1 \n            + outputs_end2 \n            + outputs_end3 \n            + outputs_end4 \n            + outputs_end5\n        ) / 5\n        \n        outputs_start = torch.softmax(outputs_start, dim=1).cpu().detach().numpy()\n        outputs_end = torch.softmax(outputs_end, dim=1).cpu().detach().numpy()\n                \n        for px, text in enumerate(orig_text):\n            extracted_text = orig_extracted[px]\n            text_label = label[px]\n            _, output_sentence, ind_start, ind_end, _ = calculate_jaccard_score(\n                original_text=text,\n                target_string=extracted_text,\n                label_val=text_label,\n                idx_start=np.argmax(outputs_start[px, :]),\n                idx_end=np.argmax(outputs_end[px, :]),\n                offsets=offsets[px]\n            )\n            final_output.append([output_sentence, ind_start, ind_end])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dfx_test[['extracted_path', 'answer_start', 'answer_end']] = final_output\nsample_submission = pd.DataFrame()\nsample_submission[['id', 'text_full', 'label', 'text', 'answer_start', 'answer_end']] = dfx_test[['id', 'text', 'label', 'extracted_path', \n                                                                                                  'answer_start', 'answer_end']]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text_bracket, answer_start_bracket, answer_end_bracket = [], [], []\n\nfor index, row in sample_submission.iterrows():\n    text_bracket.append([row.text])\n    answer_start_bracket.append([row.answer_start])\n    answer_end_bracket.append([row.answer_end])\nsample_submission['text'] = text_bracket\nsample_submission['answer_start'] = answer_start_bracket\nsample_submission['answer_end'] = answer_end_bracket\n\nsimple_json = pd.DataFrame()\nsimple_json['id'] = sample_submission['id']\nsimple_json['text'] = sample_submission['text_full']\nsimple_json['label'] = sample_submission.label\nsimple_json['extracted_part'] = sample_submission[['text', 'answer_start', 'answer_end']].to_dict(orient='records')\nsimple_json.to_json('predictions.json', orient='records', force_ascii=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"simple_json","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}